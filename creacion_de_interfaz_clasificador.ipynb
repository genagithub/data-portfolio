{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Output, Input, State\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "df_original = sns.load_dataset(\"iris\")\n",
    "\n",
    "df = df_original.copy()\n",
    "\n",
    "# codificando las variables nominales para el algoritmo\n",
    "\n",
    "df[\"species_encoded\"] = LabelEncoder().fit_transform(df[\"species\"])\n",
    "\n",
    "pca_model = PCA(n_components=2) \n",
    "\n",
    "pca = pca_model.fit_transform(df[df.columns[:4]])\n",
    "\n",
    "df[\"PCA_1\"] = pca.T[0]\n",
    "df[\"PCA_2\"] = pca.T[1]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engireering\n",
    "Es el proceso de seleccionar, eliminar y transformar las variables de un conjunto para generar una entrada de datos eficaz en un modelo específico, una popular técnica es el Análisis de Componentes Principales(PCA), cuyo fundamento se basa en conceptos de álgebra lineal y su objetivo en la reducción de la dimensionalidad pero conservando a su vez la mayor cantidad de información, proyectando una especie de sombra en los datos. Se implementa a la hora de combatir la maldición de la dimensionalidad, permitir crear gráficos cartesianos y estimular a algoritmos que no sean robustos, como en este ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(kernel=\"rbf\", C=1, probability=True)\n",
    "\n",
    "SVM.fit(pca, df[\"species_encoded\"])\n",
    "\n",
    "x_min, x_max = df[\"PCA_1\"].min() - 1, df[\"PCA_1\"].max() + 1\n",
    "y_min, y_max = df[\"PCA_2\"].min() - 1, df[\"PCA_2\"].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = SVM.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(df[\"PCA_1\"], df[\"PCA_2\"], c=df[\"species_encoded\"], s=20, edgecolor='k')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.title('Ajuste de modelo SVM con kernel radial básico y C=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Máquina de soporte vectorial\n",
    "Dado que se redujo la dimensión de características se hace más claro definir los conceptos de este algoritmo de clasificaión. Al momento de recibir los datos se define uno o más umbrales que se separan una categoría de la otra, a este umbral se lo denomina hiperplano y en este caso se traza en una distribución de puntos en 2 dimensiones, siendo también aplicado en dimensiones más grandes. Su principal elemento es el hiperplano, que es la frontera que divide todas las observaciones funcionando como una grieta entre las categorías, esto se obtiene mediante la separación a la mitad sobre la distancia entre los puntos de datos más cercanos entre una clase y otra(vectores de soporte), el objetivo final de su ecuación es generar el hiperplano óptimo, es decir, la línea divisora con el mayor margen(distancia entre los vectores de soporte y el hiperplano) posible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = df.loc[(df[\"species\"] == \"virginica\") | (df[\"species\"] == \"setosa\"), [\"PCA_1\",\"PCA_2\"]]\n",
    "var[\"PCA_1\"] = round(var[\"PCA_1\"]*100).astype(int)\n",
    "var[\"PCA_2\"] = round(var[\"PCA_2\"]*100).astype(int)\n",
    "\n",
    "def add_values(length, value_min, value_max):\n",
    "    new_values = np.array([])\n",
    "    for i in range(round(length)):\n",
    "        value = random.randint(value_min, value_max)\n",
    "        new_values = np.append(new_values, value)\n",
    "    return new_values\n",
    "    \n",
    "grp1_pca_1 = add_values(len(var)/2, 100, 400)\n",
    "grp1_pca_2 = add_values(len(var)/2, -100, 100)\n",
    "grp2_pca_1 = add_values(len(var)*1.5, -400, -200)\n",
    "grp2_pca_2 = add_values(len(var)*1.5, -100, 100)\n",
    "\n",
    "new_data_1 = np.array([grp1_pca_1,grp1_pca_2]).T\n",
    "new_data_2 = np.array([grp2_pca_1,grp2_pca_2]).T\n",
    "\n",
    "df_new_data_1 = pd.DataFrame(new_data_1, columns=[\"PCA_1\",\"PCA_2\"])\n",
    "df_new_data_2 = pd.DataFrame(new_data_2, columns=[\"PCA_1\",\"PCA_2\"])\n",
    "\n",
    "var = pd.concat([var, df_new_data_1, df_new_data_2])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(18,7))\n",
    "\n",
    "def get_function_objetive(i, data_split):\n",
    "    var[\"data_split\"] = data_split\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=150)\n",
    "    knn_regressor.fit(var.loc[var[\"data_split\"] == \"data_train\", \"PCA_2\"].values.reshape(-1,1), var.loc[var[\"data_split\"] == \"data_train\", \"PCA_1\"])\n",
    "    RMSE = root_mean_squared_error(knn_regressor.predict(var.loc[var[\"data_split\"] == \"data_test\", \"PCA_2\"].values.reshape(-1,1)), var.loc[var[\"data_split\"] == \"data_test\", \"PCA_1\"])\n",
    "    sns.scatterplot(var, x=\"PCA_2\", y=\"PCA_1\", hue=\"data_split\", ax=ax[i])\n",
    "    ax[i].grid(\"on\")\n",
    "    ax[i].set_title(f\"RMSE: {str(RMSE)[:5]}\")\n",
    "\n",
    "get_function_objetive(0, np.where(var[\"PCA_1\"] < -195, \"data_train\", \"data_test\"))\n",
    "get_function_objetive(1, np.where((var[\"PCA_1\"] <= -200) & (var[\"PCA_2\"] < 40) | (var[\"PCA_1\"] > 0) & (var[\"PCA_2\"] > 0), \"data_train\", \"data_test\"))\n",
    "get_function_objetive(2, np.where((var[\"PCA_1\"].between(-300,-200)) | (var[\"PCA_1\"].between(200,400)), \"data_train\", \"data_test\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobreajuste\n",
    "En la anterior celda se extrajeron específicamente un par de variables continuas formando un subconjunto sin contexto y que no brinda ninguna información en particular, esto se hizo para explicar un concepto recurrente en el aprendizaje automático y que está fuertemente relacionado con otra definición vista anteriormente(proyecto 2), siendo esta el error de varianza. El sobreajuste consiste en un modelo que se que se adaptó de forma elevada un grupo de datos, dejándolo poco preparado para afrontar nuevas observaciones. En la anterior figura se muestran las diferentes selecciones de datos de entrenamiento y prueba de cada modelo junto con su función objetivo que varía según las muestras. En este caso, se encuentra un umbral que diferencia dos agrupaciones con el fin de ilustrar un modelo conservador a sus objetos y como el cambio de elección de datos provoca un nuevo desempeño de este.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = [[df[\"sepal_length\"].mean(), df[\"sepal_width\"].mean(), df[\"petal_length\"].mean(), df[\"petal_width\"].mean()]]\n",
    "\n",
    "pca_object = pca_model.transform(object)\n",
    "\n",
    "# prediciendo la clase del nuevo objeto\n",
    "\n",
    "predict_encoded = SVM.predict(pca_object)\n",
    "\n",
    "# asociando las variables codificadas con sus versiones originales\n",
    "\n",
    "classes = list(zip(df[\"species\"].unique(),df[\"species_encoded\"].unique()))\n",
    "\n",
    "# asociando la predicción a su clase\n",
    "\n",
    "predict_example = classes[predict_encoded[0]][0]\n",
    "\n",
    "probability = SVM.predict_proba(pca_object)\n",
    "\n",
    "probability = probability[0, predict_encoded]*100\n",
    "\n",
    "probability = str(probability[0])\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"Nuevo objeto \\n\")\n",
    "for c in df.columns[:4]:\n",
    "    print(f\"{c}: {df[c].mean()}\")\n",
    "    \n",
    "print(f\"\\npredicción: {predict_example} | probabilidad: {probability[:4]}%\")\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard interactivo que refleja la transformación de varibles y permite introducir datos para clasificarlos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "setosa = df.loc[df[\"species\"] == \"setosa\",:]\n",
    "virginica = df.loc[df[\"species\"] == \"virginica\",:]\n",
    "versicolor = df.loc[df[\"species\"] == \"versicolor\",:]\n",
    "\n",
    "graph_pca = go.Figure()\n",
    "graph_pca.add_trace(go.Scatter(x=df[\"PCA_1\"],y=df[\"PCA_2\"],mode=\"markers\",marker_color=\"blue\",name=\"especies\"))\n",
    "graph_pca.update_layout(title=\"Figura PCA(principal components analysis)\")\n",
    "graph_pca.update_layout(legend=dict(font=dict(size=9)))\n",
    "\n",
    "predict_text = html.B(children=[],id=\"predict\",style={})\n",
    "probability_text =  html.B(children=[],id=\"probability\")\n",
    "\n",
    "colors_species = {\n",
    "    \"setosa\":\"blue\",\n",
    "    \"virginica\":\"green\",\n",
    "    \"versicolor\":\"red\"\n",
    "}\n",
    "\n",
    "app.layout =  html.Div(id=\"body\",className=\"e4_body\",children=[\n",
    "    html.H1(\"Clasificador de especies\",id=\"title\",className=\"e4_title\"),\n",
    "    html.Div(id=\"dashboard\",className=\"e4_dashboard\",children=[\n",
    "        html.Div(className=\"e4_info_div\",children=[\n",
    "           html.H2(\"Iris medias\",id=\"title_2\",className=\"e4_title_2\"),\n",
    "           html.Div(className=\"e4_info\",children=[\n",
    "              html.Img(id=\"img\",className=\"e4_img\", src=\"assets/iris.png\"),\n",
    "              html.Ul(className=\"e4_ul_div\",children=[\n",
    "              html.Ul(id=\"ul\",className=\"e4_ul\", children=[\n",
    "                html.Li(f\"longitud sétalo: {round(setosa[\"sepal_length\"].mean())}\"),\n",
    "                html.Li(f\"ancho sétalo: {round(setosa[\"sepal_width\"].mean())}\"),\n",
    "                html.Li(f\"longitud pétalo: {round(setosa[\"petal_length\"].mean())}\"),\n",
    "                html.Li(f\"ancho pétalo: {round(setosa[\"petal_width\"].mean())}\")\n",
    "              ]),\n",
    "              html.Ul(className=\"e4_ul\", children=[\n",
    "                html.Li(f\"longitud sétalo: {round(versicolor[\"sepal_length\"].mean())}\"),\n",
    "                html.Li(f\"ancho sétalo: {round(versicolor[\"sepal_width\"].mean())}\"),\n",
    "                html.Li(f\"longitud pétalo: {round(versicolor[\"petal_length\"].mean())}\"),\n",
    "                html.Li(f\"ancho pétalo: {round(versicolor[\"petal_width\"].mean())}\")\n",
    "              ]),\n",
    "              html.Ul(className=\"e4_ul\", children=[\n",
    "                html.Li(f\"longitud sétalo: {round(virginica[\"sepal_length\"].mean())}\"),\n",
    "                html.Li(f\"ancho sétalo: {round(virginica[\"sepal_width\"].mean())}\"),\n",
    "                html.Li(f\"longitud pétalo: {round(virginica[\"petal_length\"].mean())}\"),\n",
    "                html.Li(f\"ancho pétalo: {round(virginica[\"petal_width\"].mean())}\")\n",
    "              ])\n",
    "            ])\n",
    "           ])\n",
    "        ]),\n",
    "        html.Div(className=\"e4_graph_div\",children=[\n",
    "            dcc.Graph(id=\"graph-2\",className=\"e4_graph\",figure=graph_pca),\n",
    "            html.Form(id=\"input_div\",className=\"input_div\",children=[\n",
    "                dcc.Input(id=\"input_1\",className=\"input\",type=\"text\",placeholder=\"longitud_sépalo\",size=\"7\"),\n",
    "                dcc.Input(id=\"input_2\",className=\"input\",type=\"text\",placeholder=\"ancho_sépalo\",size=\"7\"),\n",
    "                dcc.Input(id=\"input_3\",className=\"input\",type=\"text\",placeholder=\"longitud_pétalo\",size=\"7\"),\n",
    "                dcc.Input(id=\"input_4\",className=\"input\",type=\"text\",placeholder=\"ancho_pétalo\",size=\"7\"),\n",
    "                html.Button(\"enviar\",id=\"button\",type=\"button\",className=\"input\",n_clicks=0)\n",
    "            ]),\n",
    "            html.P([\"predicción: \",predict_text,\" | probabildad: \",probability_text,\"%\"],className=\"e4_predict\")\n",
    "        ])\n",
    "    ])\n",
    "])\n",
    "        \n",
    "@app.callback(\n",
    "    [Output(component_id=\"graph-2\",component_property=\"figure\"),\n",
    "    Output(component_id=\"predict\",component_property=\"children\"),\n",
    "    Output(component_id=\"probability\",component_property=\"children\"),\n",
    "    Output(component_id=\"predict\",component_property=\"style\"),\n",
    "    Output(component_id=\"probability\",component_property=\"style\")],\n",
    "    [Input(component_id=\"button\",component_property=\"n_clicks\")],\n",
    "    [State(component_id=\"input_1\",component_property=\"value\"),\n",
    "    State(component_id=\"input_2\",component_property=\"value\"),\n",
    "    State(component_id=\"input_3\",component_property=\"value\"),\n",
    "    State(component_id=\"input_4\",component_property=\"value\")]\n",
    ")\n",
    "\n",
    "def update_graph(n_clicks, var_1, var_2, var_3, var_4):\n",
    "    if n_clicks is not None and n_clicks > 0:\n",
    "        object = [[var_1,var_2,var_3,var_4]]    \n",
    "        pca_object = pca_model.transform(object)\n",
    "        predict_encoded = SVM.predict(pca_object)\n",
    "        predict = classes[predict_encoded[0]][0]\n",
    "        predict_color = {\"color\":colors_species[predict]}\n",
    "        probability_color = {\"color\":colors_species[predict]}\n",
    "        probability = SVM.predict_proba(pca_object)\n",
    "        probability = probability[0,predict_encoded] * 100\n",
    "        probability = str(probability[0])\n",
    "        probability = probability[:4]\n",
    "        graph_pca.add_trace(go.Scatter(x=[pca_object[0,0]], y=[pca_object[0,1]], mode=\"markers\", marker_color=\"red\", name=f\"nueva especie({predict})\"))\n",
    "    \n",
    "    return graph_pca, predict, probability, predict_color, probability_color\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Deployment\n",
    "Hasta ahora hemos generado modelos predictivos que se encargan de resolver problemas asignados pero no fueron llevados a producción, es decir, tomar el modelo en particular desarrollado a partir de los fundamentos de Machine Learning, ponerlo a disposición de un usuario final y lograr que este modelo se pueda actualizar periódicamente para garantizar que tenga siempre el desempeño esperado. Para esto se requieren de habilidades menos científicas y más propiamente de un Ingenirero de software, donde se implementan herramientas como Docker que ayuda a crear y distribuir aplicaciones mediante contenedores que incluyan las librerías específicas instaladas, habiliten puertos, contengan volumes, etc y luego del uso de un orquestador como Kubernetes donde se gestionarán los recursos en los contenedores, servicios, configuraciones y la infraestructura en general del despliegue que se realizará. Como ya habíamos mencionado, esta área es un híbrido entre la ciencia de datos que abarca procesos como la extracción de datos, el análisis exploratorio, el preprocesamiento de datos, el entrenamiento de modelos y su evaluación y un ingeniero de softwre convencional, sin embargo, en el Machine Learning Deployment el software es mayormente dinámico, ya que el constante cambio en la información provoca que se requiera de fases posteirores al despliegue y puesta en servicio denominadas monitoreo y mantenimiento, el objetivo en estas fases es la verificación del desempeño del modelo y la repetición de las fases anteriormente mencionadas en el caso de que haya habido una degradación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
